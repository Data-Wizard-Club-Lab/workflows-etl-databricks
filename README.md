# workflows-etl-databricks
ğŸš€ DescripciÃ³n del Proyecto
Este repositorio contiene un pipeline ETL completo y modular, desarrollado en Databricks usando PySpark, que procesa el dataset Top 100 SaaS Companies 2025.

Se implementa la Medallion Architecture (Bronze âœ Silver âœ Gold) para garantizar:

Datos trazables y auditables.

Limpieza y transformaciÃ³n robusta.

Agregaciones y mÃ©tricas listas para anÃ¡lisis de negocio.

ğŸ‘‰ Compatible con Databricks Free Edition y Azure Databricks.

ğŸ“Œ Arquitectura del Pipeline
bash
Copiar
Editar
ğŸ“ FileStore/saas/
 â”œâ”€â”€ bronze/          # Ingesta cruda + tracking de metadata
 â”œâ”€â”€ silver/
 â”‚   â”œâ”€â”€ valid/       # Datos limpios y validados
 â”‚   â”œâ”€â”€ invalid/     # Registros invÃ¡lidos para auditorÃ­a
 â”œâ”€â”€ gold/
     â”œâ”€â”€ top20_by_valuation/
     â”œâ”€â”€ top20_by_arr/
     â”œâ”€â”€ by_industry/
     â”œâ”€â”€ summary_stats/
     â”œâ”€â”€ outliers/
     â”œâ”€â”€ high_g2_rating/
Cada carpeta se guarda como archivos Delta o Parquet dentro de DBFS (FileStore), simulando un data lake tipo HDFS.

âš™ï¸ Componentes
âœ… 1ï¸âƒ£ Bronze Layer â€” Ingesta cruda

Carga el CSV original.

Agrega metadata: nombre de archivo + timestamp de ingesta.

Guarda como archivos Delta en /FileStore/saas/bronze/.

âœ… 2ï¸âƒ£ Silver Layer â€” Limpieza y ValidaciÃ³n

Lee Bronze.

Limpia formatos monetarios y escala valores (M, B, T).

Convierte tipos de datos.

Valida integridad: ARR no mayor que Valuation.

Separa registros vÃ¡lidos e invÃ¡lidos.

Guarda en /FileStore/saas/silver/valid/ y /invalid/.

âœ… 3ï¸âƒ£ Gold Layer â€” Agregaciones Avanzadas

Lee Silver Valid.

Calcula:

Top 20 compaÃ±Ã­as por Valuation y ARR.

EstadÃ­sticas por industria: conteo, promedio, empleados totales.

EstadÃ­sticas de dispersiÃ³n: stddev, mediana, percentiles.

Identifica outliers (Valuation y ARR superiores al P95).

Filtra empresas con G2 Rating alto.

Guarda cada vista en subcarpetas dentro de /FileStore/saas/gold/.

ğŸ”„ OrquestaciÃ³n Recomendada
Crear 3 notebooks:

01_bronze_ingestion

02_silver_cleaning

03_gold_aggregation

Crear un Workflow en Databricks Jobs:

Task 1: Bronze âœ Task 2: Silver âœ Task 3: Gold.

Opcional: aÃ±adir notificaciones de Ã©xito/fallo por email o Slack.

Programar ejecuciÃ³n periÃ³dica (ej. diaria).

âœ… CÃ³mo Replicar
1ï¸âƒ£ Pre-requisitos

Tener cuenta en Databricks Community Edition (gratis) o acceso a Azure Databricks.

Subir el archivo CSV top_100_saas_companies_2025.csv a /FileStore/tables/.

2ï¸âƒ£ Importar Notebooks

Importar los notebooks del repositorio.

Revisar las rutas (bronze_path, silver_path, gold_path).

3ï¸âƒ£ Crear Cluster

Runtime: Databricks Runtime 11.x o superior.

Autoscaling habilitado si deseas optimizar recursos.

4ï¸âƒ£ Ejecutar Paso a Paso

Ejecutar 01_bronze_ingestion.

Luego 02_silver_cleaning.

Finalmente 03_gold_aggregation.

5ï¸âƒ£ Consultar resultados

Usar dbutils.fs.ls("dbfs:/FileStore/saas/") para explorar carpetas.

Conectar notebooks o herramientas BI para consumir las capas Gold.

ğŸ’¡ Notas
Formato: El pipeline guarda en formato Delta (Bronze, Silver) y Parquet (Gold).

Escalabilidad: Compatible con clÃºsters de producciÃ³n en Azure Databricks.

Portabilidad: Puede integrarse con ADF, Synapse o Airflow.

Costos: En Free Edition se limita a 15 GB; para grandes volÃºmenes usar cluster escalable en Azure.

ğŸ“£ Autor
Daniel Santos Pareja
ğŸš€ Data Engineer | DataHackers & Data Wizard Club
ğŸ’¬ ContÃ¡ctame para workshops, consultorÃ­a o colaboraciones.

